# -*- coding: utf-8 -*-
"""Copy of Untitled5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Fjm8VWFVobKOhN8sI963jAPEEfHqzCjH
"""

#NLP ASSIGNMENT1

import re
from collections import Counter

def clean_and_tokenize(text_data):
    text_data = text_data.lower()
    text_data = re.sub(r'\d+(\.\d+)?', '<NUMBER>', text_data)
    text_data = re.sub(r"[.,(){}[\]'\"-]", '', text_data)  # Removing unwanted punctuations
    text_data = re.sub(r'http\S+', '<WEBLINK>', text_data)
    text_data = re.sub(r'\S+@\S+', '<EMAILADDRESS>', text_data)
    token_list = text_data.split()
    token_list = ['<START_REVIEW>'] + token_list + ['<END_REVIEW>']
    return token_list

def process_reviews(file_location):
    with open(file_location, 'r') as data_file:
        reviews = data_file.readlines()

    # Apply text cleaning and tokenization to each review
    cleaned_reviews = [clean_and_tokenize(review) for review in reviews]

    return cleaned_reviews

# Function to handle tokens not in vocabulary
def replace_unknown_words(tokens_list, vocabulary):
    return [[word if word in vocabulary else "<UNKNOWN>" for word in review] for review in tokens_list]

from collections import defaultdict

def compute_unigrams_and_bigrams(corpus):
    unigram_counts = defaultdict(int)
    bigram_counts = defaultdict(int)

    # Loop through each review
    for review in corpus:
        # Update unigram counts
        for token in review:
            unigram_counts[token] += 1

    for word in list(unigram_counts.keys()):
        if unigram_counts[word] < 25:
            unigram_counts['<UNKNOWN>'] += unigram_counts[word]
            del unigram_counts[word]

    for review in corpus:
        adjusted_review = [token if unigram_counts[token] >= 25 else '<UNKNOWN>' for token in review]

        for i in range(len(adjusted_review) - 1):
            bigram = (adjusted_review[i], adjusted_review[i + 1])
            bigram_counts[bigram] += 1

    if '<UNKNOWN>' not in unigram_counts:
        unigram_counts['<UNKNOWN>'] = 0  # Initialize if an unknown word wasn't created so far

    unigram_counts = {word: count for word, count in unigram_counts.items() if count > 0}
    return unigram_counts, bigram_counts


def compute_probabilities(unigram_counts, bigram_counts):
    total_unigrams = sum(unigram_counts.values())

    # Computing unigram probabilities
    unigram_probs = {word: count / total_unigrams for word, count in unigram_counts.items()}

    # Computing bigram probabilities
    bigram_probs = {}
    for (word1, word2), count in bigram_counts.items():
        bigram_probs[(word1, word2)] = count / unigram_counts[word1]

    return unigram_probs, bigram_probs

#**implementing laplace and k smoothing.**
#logarithmic probability of tokens using Laplace smoothing for bigrams.
def laplace_bigram_smoothing(bigram_counts, unigram_counts, V, tokens):
    total_log_prob = 0.0

    for i in range(len(tokens)):
        if i > 0:
            bigram_prob = (bigram_counts.get((tokens[i-1], tokens[i]), 0) + 1) / (unigram_counts.get(tokens[i-1], 0) + V)
            total_log_prob += log(bigram_prob) if bigram_prob > 0 else log(1e-20)

    return total_log_prob

#logarithmic probability of tokens using K-smoothing for bigrams.
def k_smoothing_bigram(bigram_counts, unigram_counts, V, K, tokens):
    total_log_prob = 0.0

    for i in range(len(tokens)):
        if i > 0:
            num = bigram_counts.get((tokens[i-1], tokens[i]), 0) + K #calculating numerator and denominator values for given K value
            den = unigram_counts.get(tokens[i-1], 0) + (K * V)
            bigram_prob = num/den
            total_log_prob += log(bigram_prob) if bigram_prob > 0 else log(1e-20)

    return total_log_prob

from math import log,exp
def compute_perplexity(unigram_counts, bigram_counts, tokens,V,k=None) :
    total_log_probability_sum = 0.0;
    N = 0;

    for review in tokens:
      #for each review counting tokens
      N += len(review)
      if k is None:
            # Use Laplace smoothing
            total_review_prob = laplace_bigram_smoothing(bigram_counts, unigram_counts, V, review)
      else:
            # Use K-smoothing
            total_review_prob = k_smoothing_bigram(bigram_counts, unigram_counts, V, k, review)
      total_log_probability_sum += total_review_prob


    avg_log_probability = (-1 * total_log_probability_sum) / N
    perplexity = exp(avg_log_probability)
    #print(perplexity)
    return perplexity

def print_probabilities(unigram_counts, bigram_counts, n=None):
    unigram_probs, bigram_probs = compute_probabilities(unigram_counts, bigram_counts)
    #if n is mentioned prints those probabilities else prints all.
    print("Unigram Probabilities for training data")
    unigram_items = list(unigram_probs.items())
    for i, (word, prob) in enumerate(unigram_items):
        if n is None or i < n:
            print(f"{word}: {prob:.4f}")
        else:
            break

    print("\nBigram Probabilities for training data")
    bigram_items = list(bigram_probs.items())
    for i, ((word1, word2), prob) in enumerate(bigram_items):
        if n is None or i < n:
            print(f"({word1}, {word2}): {prob:.4f}")
        else:
            break

def main():

        train_file = 'train.txt'
        val_file = 'val.txt'
       
        clean_review_train = process_reviews(train_file)

        unigram_counts, bigram_counts = compute_unigrams_and_bigrams(clean_review_train)

        print_probabilities(unigram_counts, bigram_counts,8)
        print("\n\n")

        clean_review_val = process_reviews(val_file)
        
        clean_review_val_with_unknown = replace_unknown_words(clean_review_val, unigram_counts)
        V = len(unigram_counts) 
        if V == 0:
            print("Error: Vocabulary size is zero. Cannot perform smoothing with an empty vocabulary.")
            return
        
        laplace_perplexity = compute_perplexity(unigram_counts, bigram_counts, clean_review_val_with_unknown, V)
        
        k_values = [1, 0.1, 0.05, 0.01]
        k_smoothing_perplexities = {}
        for k in k_values:
            k_smoothing_perplexities[k] = compute_perplexity(unigram_counts, bigram_counts, clean_review_val_with_unknown, V, k)

        # perplexity results
        print(f"Perplexity with Laplace smoothing: {laplace_perplexity:.4f}")
        for k, perplexity in k_smoothing_perplexities.items():
            print(f"Perplexity with K-smoothing (K={k}): {perplexity:.4f}")
if __name__ == "__main__":
    main()